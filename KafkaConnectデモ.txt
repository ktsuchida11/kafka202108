REST APIでデータ投入できる

1.Kafka Connect 起動
connect-distributed ./connect-distributed-1.properties

2.Kafka Connect 起動確認
curl http://kafka-broker01:8083/

3.Pluglinの確認
curl http://kafka-broker01:8083/connector-plugins |  python -m json.tool

4.Connectorを起動してファイルデータをKafkaに流せるように設定する
※ファイルとトピックを結びつけている　ファイルが更新されるとKafkaへデータが投入される
echo '
{
  "name":"load-zaiko-data",
  "config":{
    "connector.class":"org.apache.kafka.connect.file.FileStreamSourceConnector",
    "file":"/zaiko/latest.txt",
    "topic":"taiko-data"
  }
}
' | curl -X POST -d @- http://kafka-broker01:8083/connectors --header "content-Type:application/json"

5.投入したデータを確認する
curl http://kafka-broker01:8083/connectors

6.投入したデータを確認する
kafka-console-consumer --bootstrap-server=kafka-broker01:9092,kafka-broker02:9092,kafka-broker03:9092 --topic zaiko-data --from-beginning

7.データを受け取るSink側の設定する
echo '
{
  "name":"sink-zaiko-data",
  "config":{
    "connector.class":"org.apache.kafka.connect.file.FileStreamSinkConnector",
    "file":"/ec/zaiko-latest.txt",
    "topics":"zaiko-data"
  }
}
' | curl -X POST -d @- http://kafka-broker01:8083/connectors --header "content-Type:application/json"


8.受け取り側のファイルを監視する
tail -f /ec/zaiko-latest.txt

9.参照元のファイルにデータを投入する
cat  << EOF >> /zaiko/latest.txt
ITEM001,SHOP001,6090,2021-07-03 03:00:00
ITEM004,SHOP001,256,2021-07-03 03:00:00

10.Connect削除
curl -X DELETE http://kafka-broker01:8083/connectors/load-zaiko-data
curl -X DELETE http://kafka-broker01:8083/connectors/skin-zaiko-data


＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝


ec_uriage Table:

 seq |     sales_time      | sales_id | item_id | amount | unit_price
-----+---------------------+----------+---------+--------+------------
   1 | 2021-07-01 01:00:00 | EC00001  | ITEM001 |      2 |    300
   2 | 2021-07-01 03:00:00 | EC00001  | ITEM002 |      2 |   5800
   3 | 2021-07-02 01:00:00 | EC00002  | ITEM003 |      2 |    298
   4 | 2021-07-02 02:00:00 | EC00002  | ITEM004 |      2 |   3000
   5 | 2021-07-02 03:00:00 | EC00002  | ITEM005 |      2 |    198


INSERT INTO ec_uriage (seq, sales_time, sales_id, item_id, amount, unit_price) VALUES ( 1, '2021-07-01 01:00:00','EC00001','ITEM001',2, 300);
INSERT INTO ec_uriage (seq, sales_time, sales_id, item_id, amount, unit_price) VALUES ( 2, '2021-07-01 03:00:00','EC00001','ITEM002',2, 5800);
INSERT INTO ec_uriage (seq, sales_time, sales_id, item_id, amount, unit_price) VALUES ( 3, '2021-07-02 01:00:00','EC00002','ITEM003',2, 298);
INSERT INTO ec_uriage (seq, sales_time, sales_id, item_id, amount, unit_price) VALUES ( 4, '2021-07-02 02:00:00','EC00002','ITEM004',2, 3000);
INSERT INTO ec_uriage (seq, sales_time, sales_id, item_id, amount, unit_price) VALUES ( 5, '2021-07-02 03:00:00','EC00002','ITEM005',2, 198);


pos_uriage
+-----+---------------------+----------+---------+---------+--------+------------+
| seq | sales_time          | sales_id | shop_id | item_id | amount | unit_price |
+-----+---------------------+----------+---------+---------+--------+------------+
|   1 | 2021-07-01 01:00:00 | POS00001 | SHOP001 | ITEM001 |      2 |        300 |
|   2 | 2021-07-01 03:00:00 | POS00001 | SHOP001 | ITEM004 |      2 |       5800 |
|   3 | 2021-07-02 01:00:00 | POS00002 | SHOP001 | ITEM005 |      2 |        298 |
|   4 | 2021-07-02 02:00:00 | POS00002 | SHOP001 | ITEM003 |      2 |       1500 |
|   5 | 2021-07-02 03:00:00 | POS00002 | SHOP001 | ITEM005 |      3 |        298 |
+-----+---------------------+----------+---------+---------+--------+------------+

NSERT INTO pos_uriage (seq, sales_time, sales_id, shop_id, item_id, amount, unit_price) VALUES ( 1, '2021-07-01 01:00:00','POS00001','SHOP001','ITEM001',2, 300);
INSERT INTO pos_uriage (seq, sales_time, sales_id, shop_id, item_id, amount, unit_price) VALUES ( 2, '2021-07-01 03:00:00','POS00001','SHOP001','ITEM004',2, 5800);
INSERT INTO pos_uriage (seq, sales_time, sales_id, shop_id, item_id, amount, unit_price) VALUES ( 3, '2021-07-02 01:00:00','POS00002','SHOP001','ITEM005',2, 298);
INSERT INTO pos_uriage (seq, sales_time, sales_id, shop_id, item_id, amount, unit_price) VALUES ( 4, '2021-07-02 02:00:00','POS00002','SHOP001','ITEM003',2, 1500);
INSERT INTO pos_uriage (seq, sales_time, sales_id, shop_id, item_id, amount, unit_price) VALUES ( 5, '2021-07-02 03:00:00','POS00002','SHOP001','ITEM005',3, 298);



$mkdir ~/.aws 
$touch ~/.aws/credentials
$chmod 600 ~/.aws/credentials
$vi ~/.aws/credentials

[default]
aws_access_key_id = AKIAR2RVJ6CDUVJ5JIRL
aws_secret_access_key = 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 


＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝
Case 2 

Zookeepr 起動  sudo systemctl start confluent-zookeeper
Kafka　　起動  sudo systemctl start confluent-kafka
Mariadb　　起動 sudo systemctl start mariadb
Pos-data-server 

Postgres　起動 sudo systemctl start postgresql-9.6 
ec2-data-server



KafkaのTopicのデータソースとしてPostgresのテーブルを登録する

echo '
{
  "name":"load-ecsales-data",
  "config":{
    "connector.class":"io.confluent.connect.jdbc.JdbcSourceConnector",
    "connection.url":"jdbc:postgresql://ec-data-server/ec",
    "connection.user":"connectuser",
    "connection.password":"connectpass",
    "mode":"incrementing",
    "incrementing.column.name":"seq",
    "table.whitelist":"ec_uriage",
    "topic.prefix":"ecsales_",
    "tasks.max":"3"
  }
}
' | curl -X POST -d @- http://kafka-broker01:8083/connectors --header "content-Type:application/json"

Topicを削除するため
curl -X DELETE http://kafka-broker01:8083/connectors/load-ecsales-data

KafkaのTopicのデータソースとしてMariaDBのテーブルを登録する
echo '
{
  "name":"load-possales-data",
  "config":{
    "connector.class":"io.confluent.connect.jdbc.JdbcSourceConnector",
    "connection.url":"jdbc:mysql://pos-data-server/pos",
    "connection.user":"connectuser",
    "connection.password":"connectpass",
    "mode":"incrementing",
    "incrementing.column.name":"seq",
    "table.whitelist":"pos_uriage",
    "topic.prefix":"possales_",
    "tasks.max":"3"
  }
}

' | curl -X POST -d @- http://kafka-broker01:8083/connectors --header "content-Type:application/json"


設定を削除する
curl -X DELETE http://kafka-broker01:8083/connectors/load-possales-data


データがKafka上に送られたことを確認する
kafka-console-consumer --bootstrap-server kafka-broker01:9092,kafka-broker02:9092,kafka-broker03:9092 --topic ecsales_ec_uriage --from-beginning


kafka-console-consumer --bootstrap-server kafka-broker01:9092,kafka-broker02:9092,kafka-broker03:9092 --topic possales_pos_uriage --from-beginning



S3をConnsumerのデータ出力先として設定する

echo '
{
  "name":"sink-sales-data",
  "config":{
    "connector.class":"io.confluent.connect.s3.S3SinkConnector",
    "s3.bucket.name":"datahub-sales01",
    "s3.region":"ap-northeast-1",
    "storage.class":"io.confluent.connect.s3.storage.S3Storage",
    "format.class":"io.confluent.connect.s3.format.json.JsonFormat",
    "flush.size":3,
    "topics":"possales_pos_uriage,ecsales_ec_uriage",
    "tasks.max":"3"
  }
}

' | curl -X POST -d @- http://kafka-broker01:8083/connectors --header "content-Type:application/json"

curl -X DELETE http://kafka-broker01:8083/connectors/sink-sales-data


Postgresへデータを追加する

INSERT INTO ec_uriage (seq, sales_time, sales_id, item_id, amount, unit_price) VALUES ( 9, '2021-07-04 01:00:00','EC00001','ITEM001',2, 400);

INSERT INTO ec_uriage (seq, sales_time, sales_id, item_id, amount, unit_price) VALUES ( 10, '2021-07-04 01:00:00','EC00001','ITEM005',6, 128);

INSERT INTO ec_uriage (seq, sales_time, sales_id, item_id, amount, unit_price) VALUES ( 11, '2021-07-04 01:00:00','EC00001','ITEM008',1, 1800);


S3にファイルができていることを確認する


MariaDBへデータを追加する

INSERT INTO pos_uriage (seq, sales_time, sales_id, shop_id, item_id, amount, unit_price) VALUES ( 10, '2021-07-04 03:00:00','POS00003','SHOP001','ITEM004',2, 800);
INSERT INTO pos_uriage (seq, sales_time, sales_id, shop_id, item_id, amount, unit_price) VALUES ( 11, '2021-07-04 01:00:00','POS00004','SHOP001','ITEM005',1, 238);
INSERT INTO pos_uriage (seq, sales_time, sales_id, shop_id, item_id, amount, unit_price) VALUES ( 12, '2021-07-04 02:00:00','POS00005','SHOP001','ITEM003',3, 1530);
INSERT INTO pos_uriage (seq, sales_time, sales_id, shop_id, item_id, amount, unit_price) VALUES ( 13, '2021-07-05 03:00:00','POS00006','SHOP001','ITEM005',4, 498);


S3にファイルができていることを確認する
